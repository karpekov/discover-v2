#!/usr/bin/env python3
"""
Visualize pre-computed text embeddings from caption files.

This script loads text embeddings (generated by Step 4), runs t-SNE,
and creates visualizations colored by activity labels.

Usage:
    python src/utils/visualize_text_embeddings.py \
        --embeddings data/processed/casas/milan/fixed_length_20/train_embeddings_baseline_gte.npz \
        --captions data/processed/casas/milan/fixed_length_20/train_captions_baseline.json \
        --output results/evals/milan/fixed_length_20/train_embeddings_baseline_gte_tsne.png \
        --max_samples 10000
"""

import sys
import json
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def extract_metadata_from_paths(embeddings_path: str) -> Dict[str, str]:
    """Extract metadata from file paths.

    Args:
        embeddings_path: Path to embeddings file

    Returns:
        Dictionary with dataset, split, is_presegmented, caption_style, encoder_name
    """
    path = Path(embeddings_path)

    # Extract dataset name from path (e.g., milan, aruba)
    parts = path.parts
    dataset_name = 'unknown'
    for i, part in enumerate(parts):
        if part == 'casas' and i + 1 < len(parts):
            dataset_name = parts[i + 1]
            break

    # Check if presegmented
    is_presegmented = 'presegmented' in str(path)

    # Extract from filename: {split}_embeddings_{style}_{encoder}.npz
    filename = path.stem  # Remove .npz
    parts = filename.split('_')

    # First part is split (train/test)
    split = parts[0] if parts else 'train'

    # Extract caption style and encoder
    # Format: train_embeddings_baseline_gte
    caption_style = 'baseline'
    encoder_name = 'gte'

    if 'embeddings' in parts:
        emb_idx = parts.index('embeddings')
        # Everything between 'embeddings' and last part is caption style
        if emb_idx + 1 < len(parts):
            # Last part is encoder
            encoder_name = parts[-1]
            # Middle parts are caption style
            if emb_idx + 2 < len(parts):
                caption_style = '_'.join(parts[emb_idx + 1:-1])
            elif emb_idx + 1 < len(parts) - 1:
                caption_style = parts[emb_idx + 1]

    return {
        'dataset_name': dataset_name,
        'split': split,
        'is_presegmented': is_presegmented,
        'caption_style': caption_style,
        'encoder_name': encoder_name
    }


def load_label_colors(dataset='milan') -> Tuple[Dict, Dict]:
    """Load label colors from metadata."""
    try:
        metadata_path = Path(__file__).parent.parent.parent / "metadata" / "casas_metadata.json"
        with open(metadata_path, 'r') as f:
            city_metadata = json.load(f)

        dataset_metadata = city_metadata.get(dataset, {})

        # Load L1 colors - try different keys in metadata
        # The correct key is 'label' for L1 activities
        label_colors = dataset_metadata.get('label', dataset_metadata.get('label_color', dataset_metadata.get('lable', {})))

        # Load L2 colors
        label_colors_l2 = dataset_metadata.get('label_deepcasas_color', {})

        print(f"ðŸŽ¨ Loaded {len(label_colors)} L1 colors, {len(label_colors_l2)} L2 colors")
        return label_colors, label_colors_l2

    except Exception as e:
        print(f"âš ï¸  Could not load label colors: {e}")
        return {}, {}


def load_embeddings_and_labels(
    embeddings_path: str,
    captions_path: str,
    data_path: str = None,
    max_samples: int = None
) -> Tuple[np.ndarray, List[str], List[str], List[str], List[str]]:
    """Load embeddings and corresponding labels.

    Args:
        embeddings_path: Path to embeddings .npz file
        captions_path: Path to captions JSON file
        data_path: Path to original data file (train.json) with labels
        max_samples: Maximum number of samples to use

    Returns:
        embeddings, sample_ids, labels_l1, labels_l2, captions
    """
    print(f"\nðŸ“– Loading embeddings from: {embeddings_path}")
    data = np.load(embeddings_path)
    embeddings = data['embeddings']
    sample_ids_from_emb = data['sample_ids']

    print(f"   Loaded {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}")
    print(f"   Encoder: {data.get('encoder_type', ['unknown'])[0]}")
    print(f"   Model: {data.get('model_name', ['unknown'])[0]}")

    # Load captions
    print(f"\nðŸ“– Loading captions from: {captions_path}")
    with open(captions_path, 'r') as f:
        captions_data = json.load(f)

    # Handle different JSON structures
    if 'captions' in captions_data and isinstance(captions_data['captions'], list):
        samples_captions = captions_data['captions']
    elif 'samples' in captions_data:
        samples_captions = captions_data['samples']
    else:
        samples_captions = captions_data

    # Build mapping from sample_id to captions
    caption_map = {}
    for sample in samples_captions:
        sample_id = sample.get('sample_id')
        if sample_id:
            caption_map[sample_id] = sample.get('captions', [''])[0]

    # Load labels from original data file
    if data_path is None:
        # Try to infer data path from captions path
        captions_path_obj = Path(captions_path)
        data_path = captions_path_obj.parent / captions_path_obj.name.replace('_captions_', '_').replace('captions_', '').replace('.json', '.json')
        if '_baseline' in str(data_path) or '_sourish' in str(data_path):
            # Extract split name
            filename = captions_path_obj.stem
            if '_captions_' in filename:
                split = filename.split('_captions_')[0]
                data_path = captions_path_obj.parent / f"{split}.json"

    print(f"\nðŸ“– Loading labels from: {data_path}")
    with open(data_path, 'r') as f:
        data_json = json.load(f)

    # Handle different data structures
    if 'samples' in data_json:
        samples_data = data_json['samples']
    else:
        samples_data = data_json

    # Build mapping from sample_id to labels
    label_map = {}
    for sample in samples_data:
        sample_id = sample.get('sample_id')
        if sample_id:
            metadata = sample.get('metadata', {})
            ground_truth = metadata.get('ground_truth_labels', {})

            # Extract L1 and L2 labels
            label_l1 = ground_truth.get('primary_l1', ground_truth.get('mode', 'Unknown'))
            label_l2 = ground_truth.get('primary_l2', 'Unknown')

            label_map[sample_id] = {
                'label_l1': label_l1,
                'label_l2': label_l2
            }

    print(f"   Loaded labels for {len(label_map)} samples")

    # Match embeddings with labels and captions
    labels_l1 = []
    labels_l2 = []
    captions = []
    valid_indices = []

    for i, sample_id in enumerate(sample_ids_from_emb):
        sample_id_str = str(sample_id)
        if sample_id_str in label_map:
            labels_l1.append(label_map[sample_id_str]['label_l1'])
            labels_l2.append(label_map[sample_id_str]['label_l2'])
            captions.append(caption_map.get(sample_id_str, ''))
            valid_indices.append(i)

    # Filter embeddings to valid indices
    embeddings = embeddings[valid_indices]
    sample_ids = [str(sample_ids_from_emb[i]) for i in valid_indices]

    print(f"   Matched {len(labels_l1)} samples with labels and captions")

    # Sample if requested
    if max_samples and len(embeddings) > max_samples:
        print(f"\nðŸŽ² Sampling {max_samples} from {len(embeddings)} samples (seed=42)")
        np.random.seed(42)
        indices = np.random.choice(len(embeddings), max_samples, replace=False)
        embeddings = embeddings[indices]
        sample_ids = [sample_ids[i] for i in indices]
        labels_l1 = [labels_l1[i] for i in indices]
        labels_l2 = [labels_l2[i] for i in indices]
        captions = [captions[i] for i in indices]

    # Print label distribution
    print(f"\nðŸ“Š Label distribution (L1):")
    label_counts = Counter(labels_l1)
    for label, count in label_counts.most_common(15):
        print(f"   {label}: {count}")

    return embeddings, sample_ids, labels_l1, labels_l2, captions


def compute_within_class_similarity(
    embeddings: np.ndarray,
    labels: List[str]
) -> Dict[str, Dict[str, float]]:
    """Compute within-class and between-class cosine similarity statistics."""
    print(f"\nðŸ“ˆ Computing similarity statistics...")

    unique_labels = sorted(set(labels))
    stats = {}

    # Compute pairwise similarities
    similarities = cosine_similarity(embeddings)

    # Overall statistics
    upper_tri_indices = np.triu_indices_from(similarities, k=1)
    all_similarities = similarities[upper_tri_indices]

    stats['overall'] = {
        'mean': float(np.mean(all_similarities)),
        'std': float(np.std(all_similarities)),
        'median': float(np.median(all_similarities)),
        'min': float(np.min(all_similarities)),
        'max': float(np.max(all_similarities))
    }

    # Per-class statistics
    for label in unique_labels:
        label_indices = [i for i, l in enumerate(labels) if l == label]

        if len(label_indices) < 2:
            continue

        # Within-class similarities
        within_class_sims = []
        for i in range(len(label_indices)):
            for j in range(i + 1, len(label_indices)):
                idx_i = label_indices[i]
                idx_j = label_indices[j]
                within_class_sims.append(similarities[idx_i, idx_j])

        # Between-class similarities
        between_class_sims = []
        other_indices = [i for i, l in enumerate(labels) if l != label]
        for idx_i in label_indices[:min(100, len(label_indices))]:  # Sample to avoid memory issues
            for idx_j in other_indices[:min(100, len(other_indices))]:
                between_class_sims.append(similarities[idx_i, idx_j])

        if within_class_sims:
            stats[label] = {
                'count': len(label_indices),
                'within_class_mean': float(np.mean(within_class_sims)),
                'within_class_std': float(np.std(within_class_sims)),
                'between_class_mean': float(np.mean(between_class_sims)) if between_class_sims else 0.0,
                'separation': float(np.mean(within_class_sims) - np.mean(between_class_sims)) if between_class_sims else 0.0
            }

    # Print summary
    print(f"\n{'='*70}")
    print("SIMILARITY STATISTICS")
    print(f"{'='*70}")
    print(f"\nOverall:")
    print(f"  Mean similarity: {stats['overall']['mean']:.4f}")
    print(f"  Std similarity: {stats['overall']['std']:.4f}")
    print(f"  Median similarity: {stats['overall']['median']:.4f}")

    print(f"\nPer-class statistics (top 10 by count):")
    print(f"{'Label':<25} {'Count':>6} {'Within':>8} {'Between':>8} {'Sep':>8}")
    print(f"{'-'*70}")

    sorted_labels = sorted(
        [(l, s) for l, s in stats.items() if l != 'overall' and 'count' in s],
        key=lambda x: x[1]['count'],
        reverse=True
    )[:10]

    for label, stat in sorted_labels:
        print(f"{label:<25} {stat['count']:>6} "
              f"{stat['within_class_mean']:>8.4f} "
              f"{stat['between_class_mean']:>8.4f} "
              f"{stat['separation']:>8.4f}")

    print(f"{'='*70}\n")

    return stats


def run_tsne(
    embeddings: np.ndarray,
    perplexity: int = 30,
    random_state: int = 42
) -> np.ndarray:
    """Run t-SNE dimensionality reduction."""
    print(f"\nðŸ”„ Running t-SNE (perplexity={perplexity})...")

    tsne = TSNE(
        n_components=2,
        perplexity=min(perplexity, len(embeddings) // 4),
        random_state=random_state,
        max_iter=1000,
        verbose=1
    )

    projection = tsne.fit_transform(embeddings)
    print(f"âœ… t-SNE completed: {projection.shape}")

    return projection


def create_visualization(
    projection: np.ndarray,
    labels_l1: List[str],
    labels_l2: List[str],
    label_colors: Dict[str, str],
    label_colors_l2: Dict[str, str],
    save_path: str,
    dataset_name: str = 'milan',
    split: str = 'train',
    is_presegmented: bool = False,
    caption_style: str = 'baseline',
    encoder_name: str = 'gte'
):
    """Create t-SNE visualization with L1 and L2 labels."""
    print(f"\nðŸŽ¨ Creating visualization...")

    # Set up the plot
    plt.style.use('default')
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

    # Plot 1: L1 activities
    unique_labels_l1 = sorted(set(labels_l1))

    for label in unique_labels_l1:
        mask = np.array(labels_l1) == label

        # Get color from metadata or use default
        if label in label_colors:
            color = label_colors[label]
        else:
            color = plt.cm.tab20(len([l for l in unique_labels_l1 if l < label]) % 20)

        ax1.scatter(
            projection[mask, 0],
            projection[mask, 1],
            c=[color],
            label=label.replace('_', ' '),
            alpha=0.6,
            s=30,
            edgecolors='white',
            linewidth=0.5
        )

    ax1.set_xlabel('t-SNE 1', fontsize=12)
    ax1.set_ylabel('t-SNE 2', fontsize=12)

    # Format titles
    preseg_text = "presegmented" if is_presegmented else "standard"
    title_l1 = f'Caption Emb, {dataset_name.capitalize()} ({split} - {preseg_text}), L1 labels'
    subtitle_l1 = f'{caption_style.capitalize()} Captions, {encoder_name.upper()} Encoder'

    ax1.set_title(f'{title_l1}\n{subtitle_l1}',
                  fontsize=14, fontweight='bold')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax1.grid(True, alpha=0.3)

    # Plot 2: L2 activities
    unique_labels_l2 = sorted(set(labels_l2))

    for label in unique_labels_l2:
        mask = np.array(labels_l2) == label

        # Get color from metadata or use default
        if label in label_colors_l2:
            color = label_colors_l2[label]
        else:
            color = plt.cm.tab10(len([l for l in unique_labels_l2 if l < label]) % 10)

        ax2.scatter(
            projection[mask, 0],
            projection[mask, 1],
            c=[color],
            label=label.replace('_', ' '),
            alpha=0.6,
            s=30,
            edgecolors='white',
            linewidth=0.5
        )

    ax2.set_xlabel('t-SNE 1', fontsize=12)
    ax2.set_ylabel('t-SNE 2', fontsize=12)

    # Format titles for L2
    title_l2 = f'Caption Emb, {dataset_name.capitalize()} ({split} - {preseg_text}), L2 labels'
    subtitle_l2 = f'{caption_style.capitalize()} Captions, {encoder_name.upper()} Encoder'

    ax2.set_title(f'{title_l2}\n{subtitle_l2}',
                  fontsize=14, fontweight='bold')
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ðŸ’¾ Visualization saved to: {save_path}")

    plt.close()


def save_statistics(stats: Dict, output_path: str):
    """Save similarity statistics to JSON."""
    stats_path = Path(output_path).parent / (Path(output_path).stem + '_stats.json')
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"ðŸ’¾ Statistics saved to: {stats_path}")


def create_comparison_grid(
    embeddings_dir: str,
    captions_path: str,
    data_path: str,
    output_prefix: str,
    max_samples: int = 10000,
    perplexity: int = 30
):
    """Create comparison grids for all embeddings in a directory.

    Args:
        embeddings_dir: Directory containing embedding files
        captions_path: Path to captions file
        data_path: Path to data file with labels
        output_prefix: Prefix for output files
        max_samples: Maximum samples to visualize
        perplexity: t-SNE perplexity
    """
    from glob import glob
    import math

    print("\n" + "="*80)
    print("CREATING COMPARISON GRID FOR ALL EMBEDDINGS")
    print("="*80)

    # Find all embedding files
    embeddings_dir = Path(embeddings_dir)
    embedding_files = sorted(embeddings_dir.glob("*_embeddings_*.npz"))

    if not embedding_files:
        print(f"âŒ No embedding files found in {embeddings_dir}")
        return

    print(f"\nðŸ“‚ Found {len(embedding_files)} embedding files:")
    for f in embedding_files:
        print(f"   - {f.name}")

    # Extract metadata from first file to get dataset info
    first_metadata = extract_metadata_from_paths(str(embedding_files[0]))
    dataset_name = first_metadata['dataset_name']

    # Load label colors
    label_colors, label_colors_l2 = load_label_colors(dataset_name)

    # Load data and captions once (without sampling)
    print(f"\nðŸ“– Loading captions and labels...")
    _, sample_ids, labels_l1_all, labels_l2_all, captions = load_embeddings_and_labels(
        str(embedding_files[0]),  # Use first file to get labels
        captions_path,
        data_path=data_path,
        max_samples=None  # Don't sample yet
    )

    # Sample indices once for consistency across all embeddings
    if max_samples and len(labels_l1_all) > max_samples:
        print(f"\nðŸŽ² Sampling {max_samples} from {len(labels_l1_all)} samples (seed=42)")
        np.random.seed(42)
        sample_indices = np.random.choice(len(labels_l1_all), max_samples, replace=False)
        labels_l1 = [labels_l1_all[i] for i in sample_indices]
        labels_l2 = [labels_l2_all[i] for i in sample_indices]
    else:
        sample_indices = None
        labels_l1 = labels_l1_all
        labels_l2 = labels_l2_all

    # Calculate grid dimensions
    n_encoders = len(embedding_files)
    n_cols = min(3, n_encoders)  # Max 3 columns
    n_rows = math.ceil(n_encoders / n_cols)

    # Create figures for L1 and L2
    fig_l1, axes_l1 = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 7*n_rows))
    fig_l2, axes_l2 = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 7*n_rows))

    # Flatten axes for easier indexing
    if n_encoders == 1:
        axes_l1 = [axes_l1]
        axes_l2 = [axes_l2]
    else:
        axes_l1 = axes_l1.flatten() if n_encoders > 1 else [axes_l1]
        axes_l2 = axes_l2.flatten() if n_encoders > 1 else [axes_l2]

    # Process each embedding file
    for idx, emb_file in enumerate(embedding_files):
        print(f"\nðŸ”„ Processing {emb_file.name}...")

        # Extract metadata
        metadata = extract_metadata_from_paths(str(emb_file))
        encoder_name = metadata['encoder_name'].upper()

        # Load embeddings
        data = np.load(emb_file)
        embeddings = data['embeddings']

        # Apply same sampling
        if sample_indices is not None:
            embeddings = embeddings[sample_indices]

        # Run t-SNE
        print(f"   Running t-SNE...")
        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, verbose=0)
        projection = tsne.fit_transform(embeddings)

        # Plot L1
        ax_l1 = axes_l1[idx]
        unique_labels_l1 = sorted(set(labels_l1))
        for label in unique_labels_l1:
            mask = np.array(labels_l1) == label
            color = label_colors.get(label, plt.cm.tab20(len([l for l in unique_labels_l1 if l < label]) % 20))
            ax_l1.scatter(
                projection[mask, 0],
                projection[mask, 1],
                c=[color],
                label=label.replace('_', ' '),
                alpha=0.6,
                s=20,
                edgecolors='white',
                linewidth=0.3
            )
        ax_l1.set_title(f'{encoder_name}', fontsize=12, fontweight='bold')
        ax_l1.set_xlabel('t-SNE 1', fontsize=10)
        ax_l1.set_ylabel('t-SNE 2', fontsize=10)
        ax_l1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
        ax_l1.grid(True, alpha=0.3)

        # Plot L2
        ax_l2 = axes_l2[idx]
        unique_labels_l2 = sorted(set(labels_l2))
        for label in unique_labels_l2:
            mask = np.array(labels_l2) == label
            color = label_colors_l2.get(label, plt.cm.tab10(len([l for l in unique_labels_l2 if l < label]) % 10))
            ax_l2.scatter(
                projection[mask, 0],
                projection[mask, 1],
                c=[color],
                label=label.replace('_', ' '),
                alpha=0.6,
                s=20,
                edgecolors='white',
                linewidth=0.3
            )
        ax_l2.set_title(f'{encoder_name}', fontsize=12, fontweight='bold')
        ax_l2.set_xlabel('t-SNE 1', fontsize=10)
        ax_l2.set_ylabel('t-SNE 2', fontsize=10)
        ax_l2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
        ax_l2.grid(True, alpha=0.3)

    # Hide unused subplots
    for idx in range(n_encoders, len(axes_l1)):
        axes_l1[idx].set_visible(False)
        axes_l2[idx].set_visible(False)

    # Add overall titles
    preseg_text = "presegmented" if first_metadata['is_presegmented'] else "standard"
    split = first_metadata['split']
    caption_style = first_metadata['caption_style']

    fig_l1.suptitle(
        f'Embedding Comparison: {dataset_name.capitalize()} ({split} - {preseg_text}), L1 Labels\n'
        f'{caption_style.capitalize()} Captions - All Encoders',
        fontsize=16,
        fontweight='bold',
        y=0.995
    )

    fig_l2.suptitle(
        f'Embedding Comparison: {dataset_name.capitalize()} ({split} - {preseg_text}), L2 Labels\n'
        f'{caption_style.capitalize()} Captions - All Encoders',
        fontsize=16,
        fontweight='bold',
        y=0.995
    )

    # Adjust layout
    fig_l1.tight_layout()
    fig_l2.tight_layout()

    # Save with lower DPI for smaller file size
    output_l1 = f"{output_prefix}_comparison_l1.png"
    output_l2 = f"{output_prefix}_comparison_l2.png"

    fig_l1.savefig(output_l1, dpi=150, bbox_inches='tight')
    fig_l2.savefig(output_l2, dpi=150, bbox_inches='tight')

    print(f"\nðŸ’¾ Saved L1 comparison to: {output_l1}")
    print(f"ðŸ’¾ Saved L2 comparison to: {output_l2}")

    plt.close(fig_l1)
    plt.close(fig_l2)


def main():
    parser = argparse.ArgumentParser(
        description='Visualize pre-computed text embeddings with t-SNE'
    )

    parser.add_argument(
        '--embeddings',
        type=str,
        help='Path to embeddings .npz file (for single visualization)'
    )

    parser.add_argument(
        '--embeddings-dir',
        type=str,
        help='Directory containing multiple embedding files (for comparison grid)'
    )

    parser.add_argument(
        '--captions',
        type=str,
        required=True,
        help='Path to captions JSON file'
    )

    parser.add_argument(
        '--data',
        type=str,
        help='Path to original data file (train.json) with labels. If not provided, will try to infer from captions path.'
    )

    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='Path to save visualization PNG (or prefix for comparison grid)'
    )

    parser.add_argument(
        '--max-samples',
        type=int,
        default=10000,
        help='Maximum number of samples to visualize (default: 10000)'
    )

    parser.add_argument(
        '--perplexity',
        type=int,
        default=30,
        help='t-SNE perplexity parameter (default: 30)'
    )

    args = parser.parse_args()

    # Check if we're doing comparison or single visualization
    if args.embeddings_dir:
        # Comparison mode
        create_comparison_grid(
            embeddings_dir=args.embeddings_dir,
            captions_path=args.captions,
            data_path=args.data,
            output_prefix=args.output,
            max_samples=args.max_samples,
            perplexity=args.perplexity
        )
        print("\n" + "="*80)
        print("âœ¨ Comparison grid complete!")
        print("="*80 + "\n")

    elif args.embeddings:
        # Single visualization mode
        print("="*80)
        print("TEXT EMBEDDING VISUALIZATION")
        print("="*80)

        # Extract metadata from paths
        metadata = extract_metadata_from_paths(args.embeddings)
        print(f"\nðŸ“‹ Extracted metadata:")
        print(f"   Dataset: {metadata['dataset_name']}")
        print(f"   Split: {metadata['split']}")
        print(f"   Presegmented: {metadata['is_presegmented']}")
        print(f"   Caption style: {metadata['caption_style']}")
        print(f"   Encoder: {metadata['encoder_name']}")

        # Load label colors
        label_colors, label_colors_l2 = load_label_colors(metadata['dataset_name'])

        # Load embeddings and labels
        embeddings, sample_ids, labels_l1, labels_l2, captions = load_embeddings_and_labels(
            args.embeddings,
            args.captions,
            data_path=args.data,
            max_samples=args.max_samples
        )

        # Compute similarity statistics
        stats = compute_within_class_similarity(embeddings, labels_l1)

        # Run t-SNE
        projection = run_tsne(embeddings, perplexity=args.perplexity)

        # Create visualization
        create_visualization(
            projection,
            labels_l1,
            labels_l2,
            label_colors,
            label_colors_l2,
            args.output,
            dataset_name=metadata['dataset_name'],
            split=metadata['split'],
            is_presegmented=metadata['is_presegmented'],
            caption_style=metadata['caption_style'],
            encoder_name=metadata['encoder_name']
        )

        # Save statistics
        save_statistics(stats, args.output)

        print("\n" + "="*80)
        print("âœ¨ Visualization complete!")
        print("="*80 + "\n")

    else:
        parser.error("Either --embeddings or --embeddings-dir must be provided")


if __name__ == '__main__':
    main()

